@ARTICLE{Cook1982-eq,
  title   = "Residuals and inﬂuence in regression",
  author  = "Cook, R Dennis and Weisberg, Sanford",
  journal = "Monographs on Statistics and Applied Probability",
  year    =  1982
}

@BOOK{Gelman2013-st,
  title     = "Bayesian Data Analysis, Third Edition",
  author    = "Gelman, A and Carlin, JB and Stern, HS and Dunson, DB and
               Vehtari, A and Rubin, DB",
  abstract  = "Now in its third edition, this classic book is widely considered
               the leading text on Bayesian methods, lauded for its accessible,
               practical approach to analyzing data and solving research
               problems. Bayesian Data Analysis , Third Edition continues to
               take an applied ...",
  publisher = "Taylor \& Francis",
  series    = "Chapman \& Hall/CRC Texts in Statistical Science",
  year      =  2013
}

@ARTICLE{OHagan2003-bc,
  title     = "{HSSS} Model Criticism",
  author    = "O'Hagan, A",
  abstract  = "There has been an enormous surge in practical applications of
               Bayesian statistics in the last decade. The primary reason for
               this has been the development of MCMC tools for computing
               posterior inferences. HSSS has played a major part in that
               process, through helping to ...",
  journal   = "Highly Structured Stochastic Systems",
  publisher = "Oxford University Press",
  pages     = "423--444",
  year      =  2003
}


@INPROCEEDINGS{Lloyd2014-ABCD,
  title         = "Automatic Construction and {Natural-Language} Description of
                   Nonparametric Regression Models",
  author        = {Lloyd, James Robert and Duvenaud, David and Grosse, Roger
                   and Tenenbaum, Joshua B. and Ghahramani, Zoubin},
  booktitle     = "Association for the Advancement of Artificial Intelligence
                   (AAAI)",
  year          = 2014
}


@INPROCEEDINGS{Grosse2012-zf,
  title      = "Exploiting compositionality to explore a large space of model
                structures",
  author     = "Grosse, Roger and Salakhutdinov, Ruslan R and Freeman, William
                T. and Tenenbaum, Joshua B.",
  abstract   = "The recent proliferation of richly structured probabilistic
                models raises the question of how to automatically determine an
                appropriate model for a dataset. We investigate this question
                for a space of matrix decomposition models which can express a
                variety of widely used models from unsupervised learning. To
                enable model selection, we organize these models into a
                context-free grammar which generates a wide variety of
                structures through the compositional application of a few
                simple rules. We use our grammar to generically and efficiently
                infer latent components and estimate predictive likelihood for
                nearly 2500 structures using a small toolbox of reusable
                algorithms. Using a greedy search over our grammar, we
                automatically choose the decomposition structure from raw data
                by evaluating only a small fraction of all models. The proposed
                method typically finds the correct structure for synthetic data
                and backs off gracefully to simpler models under heavy noise.
                It learns sensible structures for datasets as diverse as image
                patches, motion capture, 20 Questions, and U.S. Senate votes,
                all using exactly the same code.",
  year       =  2012,
  journal    = "Uncertainty in Artificial Intelligence"
}

@INPROCEEDINGS{Thornton2013-zg,
  title     = "{Auto-WEKA}: Combined Selection and Hyperparameter Optimization
               of Classification Algorithms",
  booktitle = "Proc. of 19th {ACM} {SIGKDD} Int. Conf.
               on Knowledge Discovery and Data Mining",
  author    = "Thornton, Chris and Hutter, Frank and Hoos, Holger H. and
               Leyton-Brown, Kevin",
  abstract  = "Abstract Many different machine learning algorithms exist;
               taking into account each algorithm's hyperparameters, there is a
               staggeringly large number of possible alternatives overall. We
               consider the problem of simultaneously selecting a learning
               algorithm and ...",
  year      =  2013,
  keywords  = "hyperparameter optimization, model selection, weka"
}

@misc{stan-software:2014,
  author = {{Stan Development Team}},
  year = {2014},
  title = {Stan: A C++ Library for Probability and Sampling,
           Version 2.2},
  url = {http://mc-stan.org/}
}

@INPROCEEDINGS{Goodman2012-pf,
  title         = "Church: a language for generative models",
  booktitle     = "Uncertainty in Artificial Intelligence ({UAI})",
  author        = "Goodman, Noah and Mansinghka, Vikash and Roy, Daniel and
                   Bonawitz, Keith and Tarlow, Daniel",
  abstract      = "We introduce Church, a universal language for describing
                   stochastic generative processes. Church is based on the Lisp
                   model of lambda calculus, containing a pure Lisp as its
                   deterministic subset. The semantics of Church is defined in
                   terms of evaluation histories and conditional distributions
                   on such histories. Church also includes a novel language
                   construct, the stochastic memoizer, which enables simple
                   description of many complex non-parametric models. We
                   illustrate language features through several examples,
                   including: a generalized Bayes net in which parameters
                   cluster over trials, infinite PCFGs, planning by inference,
                   and various non-parametric clustering models. Finally, we
                   show how to implement query on any Church program, exactly
                   and approximately, using Monte Carlo techniques.",
  journal       = "arXiv [cs.PL]",
  publisher     = "arxiv.org",
  month         =  06,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@INPROCEEDINGS{Meulders1998-xo,
  title     = "Generalizing the probability matrix decomposition model: an
               example of Bayesian model checking and model expansion",
  booktitle = "Assumptions, Robustness, and Estimation Methods in Multivariate
               Modeling",
  author    = "Meulders, Michel and Gelman, Andrew and Van Mechelen, Iven and
               De Boeck, Paul",
  abstract  = "CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep
               Teregowda): Probability matrix decomposition (PMD) models can be
               used to explain observed associations between two sets of
               elements. More specifically, observed associations are modeled
               as a deterministic function of B latent Bernoulli variables that
               are realized for each element. To estimate the parameters of
               this model, a sample of the posterior distribution is computed
               with a data augmentation algorithm. The obtained posterior
               sample can also be used to assess the fit of the model with the
               technique of posterior predictive checks. In this paper a PMD
               model is applied to data on psychiatric diagnosis. In checking
               the model for this analysis, we focus on the appropriateness of
               the prior distribution for a set of latent parameters. Based on
               the posterior distribution for the values of the parameters
               corresponding to the observed data, we conclude that a
               relatively flat prior distribution is inappropriate. In order to
               solve this problem, a mixture prior density with two beta
               distributed components ...",
  publisher = "Citeseer",
  year      =  1998
}

@ARTICLE{Gretton2008-ik,
  title    = "A kernel method for the two-sample-problem",
  author   = "Gretton, A and Borgwardt, KM and Rasch, M and Sch{\"{o}}lkopf, B
              and Smola, A",
  abstract = "Abstract We propose two statistical tests to determine if two
              samples are from different distributions. Our test statistic is
              in both cases the distance between the means of the two samples
              mapped into a reproducing kernel Hilbert space (RKHS). The first
              test is based ...",
  journal  = "Journal of Machine Learning Research",
  year     =  2008
}

@ARTICLE{Rubin1984-tw,
  title     = "Bayesianly Justifiable and Relevant Frequency Calculations for
               the Applied Statistician",
  author    = "Rubin, Donald B.",
  journal   = "The Annals of Statistics",
  publisher = "Institute of Mathematical Statistics",
  volume    =  12,
  number    =  4,
  pages     = "1151--1172",
  month     =  dec,
  year      =  1984,
  keywords  = "62-07; Calibration; empirical Bayes; inference; model
               monitoring; operating characteristics; posterior predictive
               checks; stopping rules",
  issn_alt  = "2168-8966"
}


@UNPUBLISHED{Orbanz2013-cm,
  title         = "Bayesian Models of Graphs, Arrays and Other Exchangeable
                   Random Structures",
  author        = "Orbanz, Peter and Roy, Daniel M.",
  abstract      = "The natural habitat of most Bayesian methods is data
                   represented by exchangeable sequences of observations, for
                   which de Finetti's theorem provides the theoretical
                   foundation. Dirichlet process clustering, Gaussian process
                   regression, and many other parametric and nonparametric
                   Bayesian models fall within the remit of this framework;
                   many problems arising in modern data analysis do not. This
                   expository paper provides an introduction to Bayesian models
                   of graphs, matrices, and other data that can be modeled by
                   random structures. We describe results in probability theory
                   that generalize de Finetti's theorem to such data and
                   discuss the relevance of these results to nonparametric
                   Bayesian modeling. With the basic ideas in place, we survey
                   example models available in the literature; applications of
                   such models include collaborative filtering, link
                   prediction, and graph and network analysis. We also
                   highlight connections to recent developments in graph theory
                   and probability, and sketch the more general mathematical
                   foundation of Bayesian methods for other types of data
                   beyond sequences and arrays.",
  publisher     = "arxiv.org",
  month         =  "30~" # dec,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "1312.7857"
}


@ARTICLE{Friedman1979-ur,
  title     = "Multivariate Generalizations of the {Wald-Wolfowitz} and Smirnov
               {Two-Sample} Tests",
  author    = "Friedman, Jerome H. and Rafsky, Lawrence C.",
  abstract  = "Multivariate generalizations of the Wald-Wolfowitz runs
               statistic and the Smirnov maximum deviation statistic for the
               two-sample problem are presented. They are based on the minimal
               spanning tree of the pooled sample points. Some null
               distribution results are derived and a simulation study of power
               is reported.",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  7,
  number    =  4,
  pages     = "697--717",
  month     =  "1~" # jul,
  year      =  1979
}

@ARTICLE{Bickel1969-ao,
  title     = "A Distribution Free Version of the Smirnov Two Sample Test in
               the p-Variate Case",
  author    = "Bickel, P J",
  abstract  = "1. Introduction. One of the classic problems of the theory of
               nonparametric inference is testing whether two samples come from
               the same or different populations. If the observations are
               univariate and we suppose only that the parent populations are
               governed by a ...",
  journal   = "Ann. Math. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  40,
  number    =  1,
  pages     = "1--23",
  month     =  "1~" # feb,
  year      =  1969
}

@INPROCEEDINGS{Hotelling1951-jd,
  title     = "A Generalized {t-Test} and Measure of Multivariate Dispersion",
  booktitle = "Proc. of the 2nd Berkeley Symp. on Math.
               Statistics and Probability",
  author    = "Hotelling, Harold",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Proceedings of the second Berkeley symposium",
  year      =  1951,
  keywords  = "Hotelling T-squared distribution"
}

@ARTICLE{Muller1997-vs,
  title     = "Integral Probability Metrics and Their Generating Classes of
               Functions",
  author    = "M{\"{u}}ller, Alfred",
  abstract  = "We consider probability metrics of the following type: for a
               class F of functions and probability measures P, Q we define
               $d\_\textbackslash{}germF(P,\textbackslash{}
               Q)\textbackslash{}coloneq
               \textbackslash{}textsup\_f\textbackslash{}in
               \textbackslash{}germF|\textbackslash{}int
               f\textbackslash{},dP-\textbackslash{}int f\textbackslash{},dQ|$
               . A unified study of such integral probability metrics is given.
               We characterize the maximal class of functions that generates
               such a metric. Further, we show how some interesting properties
               of these probability metrics arise directly from conditions on
               the generating class of functions. The results are illustrated
               by several examples, including the Kolmogorov metric, the Dudley
               metric and the stop-loss metric.",
  journal   = "Adv. Appl. Probab.",
  publisher = "Applied Probability Trust",
  volume    =  29,
  number    =  2,
  pages     = "429--443",
  month     =  "1~" # jun,
  year      =  1997
}

@BOOK{Rasmussen2006-ml,
  title     = "Gaussian Processes for Machine Learning",
  author    = "Rasmussen, CE and Williams, CKI",
  publisher = "The MIT Press, Cambridge, MA, USA",
  year      =  2006
}

@ARTICLE{Stigler1977-dd,
  title     = "Do Robust Estimators Work with Real Data?",
  author    = "Stigler, Stephen M.",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  5,
  number    =  6,
  pages     = "1055--1098",
  month     =  nov,
  year      =  1977,
  keywords  = "62-02; $M$-estimators; trimmed means; simulation; Monte Carlo;
               median; bias; adaptive estimators; skewness; kurtosis",
  issn_alt  = "2168-8966"
}

@UNPUBLISHED{Iwata2012-yj,
  title         = "Warped Mixtures for Nonparametric Cluster Shapes",
  author        = "Iwata, Tomoharu and Duvenaud, David and Ghahramani, Zoubin",
  abstract      = "A mixture of Gaussians fit to a single curved or
                   heavy-tailed cluster will report that the data contains many
                   clusters. To produce more appropriate clusterings, we
                   introduce a model which warps a latent mixture of Gaussians
                   to produce nonparametric cluster shapes. The possibly
                   low-dimensional latent mixture model allows us to summarize
                   the properties of the high-dimensional clusters (or density
                   manifolds) describing the data. The number of manifolds, as
                   well as the shape and dimension of each manifold is
                   automatically inferred. We derive a simple inference scheme
                   for this model which analytically integrates out both the
                   mixture parameters and the warping function. We show that
                   our model is effective for density estimation, performs
                   better than infinite Gaussian mixture models at recovering
                   the true number of clusters, and produces interpretable
                   summaries of high-dimensional datasets.",
  publisher     = "arxiv.org",
  month         =  06,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1206.1846"
}

@ARTICLE{Peel2000-pv,
  title     = "Robust mixture modelling using the t distribution",
  author    = "Peel, D and McLachlan, G J",
  abstract  = "Normal mixture models are being increasingly used to model the
               distributions of a wide variety of random phenomena and to
               cluster sets of continuous multivariate data. However, for a set
               of data containing a group or groups of observations with longer
               than normal tails or atypical observations, the use of normal
               components may unduly affect the fit of the mixture model. In
               this paper, we consider a more robust approach by modelling the
               data by a mixture of t distributions. The use of the ECM
               algorithm to fit this t mixture model is described and examples
               of its use are given in the context of clustering multivariate
               data in the presence of atypical observations in the form of
               background noise.",
  journal   = "Stat. Comput.",
  publisher = "Kluwer Academic Publishers",
  volume    =  10,
  number    =  4,
  pages     = "339--348",
  month     =  "1~" # oct,
  year      =  2000,
  issn_alt  = "1573-1375"
}

@ARTICLE{Hinton2007-eo,
  title   = "To Recognize Shapes , First Learn to Generate Images",
  author  = "Hinton, Geoffrey E.",
  journal = "Prog. Brain Res.",
  volume  =  165,
  pages   = "535--547",
  year    =  2007
}

@ARTICLE{Hinton2006-yw,
  title   = "A fast learning algorithm for deep belief nets",
  author  = "Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye",
  journal = "Neural Comput.",
  volume  =  18,
  number  =  7,
  pages   = "1527--1554",
  year    =  2006
}

@ARTICLE{Gelman2013-am,
  title    = "Understanding posterior p-values",
  author   = "Gelman, Andrew",
  journal  = "Electronic Journal of Statistics",
  pages    = "1--8",
  year     =  2013,
  keywords = "and phrases; bayesian inference; model checking; p-value;
              posterior; predictive check; u-value"
}

@ARTICLE{Robins2000-oz,
  title    = "Asymptotic Distribution of p Values in Composite Null Models",
  author   = "Robins, James M. and van der Vaart, Aad and Venture, Valerie",
  journal  = "Journal of the American Statistical Association",
  volume   =  95,
  number   =  452,
  pages    = "1143--1156",
  year     =  2000,
  keywords = "asymptotic"
}

@ARTICLE{Bayarri1999-ty,
  title   = "Quantifying surprise in the data and model verification",
  author  = "Bayarri, MJ and Berger, JO",
  journal = "Bayesian statistics",
  year    =  1999
}

@ARTICLE{Box1980-ud,
  title     = "Sampling and Bayes' Inference in Scientific Modelling and
               Robustness",
  author    = "Box, George E. P.",
  abstract  = "Scientific learning is an iterative process employing Criticism
               and Estimation. Correspondingly the formulated model factors
               into two complementary parts--a predictive part allowing model
               criticism, and a Bayes posterior part allowing estimation.
               Implications for significance tests, the theory of precise
               measurement and for ridge estimates are considered. Predictive
               checking functions for transformation, serial correlation, bad
               values, and their relation with Bayesian options are considered.
               Robustness is seen from a Bayesian viewpoint and examples are
               given. For the bad value problem a comparison with M estimators
               is made.",
  journal   = "J. R. Stat. Soc. Ser. A",
  publisher = "Wiley for the Royal Statistical Society",
  volume    =  143,
  number    =  4,
  pages     = "383--430",
  month     =  "1~" # jan,
  year      =  1980
}

@ARTICLE{Johnson2004-ej,
  title    = "A Bayesian chi squared test for goodness-of-fit",
  author   = "Johnson, Valen E.",
  journal  = "Ann. Stat.",
  volume   =  32,
  number   =  6,
  pages    = "2361--2384",
  month    =  dec,
  year     =  2004,
  keywords = "and phrases; bayes factor; bayesian model assessment;
              discrepancy; intrinsic bayes factor; p -value; pearson;
              posterior-predictive diagnostics; s chi-squared statistic"
}

@ARTICLE{Dey1998-dn,
  title   = "A simulation-intensive approach for checking hierarchical models",
  author  = "Dey, Dipak K. and Gelfand, Alan E. and Swartz, Tim B. and Vlachos,
             Pantelis K.",
  journal = "Test",
  volume  =  7,
  number  =  2,
  pages   = "325--346",
  month   =  dec,
  year    =  1998
}

@ARTICLE{Marshall2007-hd,
  title     = "Identifying outliers in Bayesian hierarchical models: a
               simulation-based approach",
  author    = "Marshall, E C and Spiegelhalter, D J",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Bayesian Anal.",
  publisher = "International Society for Bayesian Analysis",
  volume    =  2,
  number    =  2,
  pages     = "409--444",
  month     =  jun,
  year      =  2007,
  keywords  = "Hierarchical models; Diagnostics; Outliers; Distributional
               assumptions",
  issn_alt  = "1931-6690"
}

@INCOLLECTION{Gelfand1996-vy,
  title     = "Model determination using sampling-based methods",
  booktitle = "Markov Chain Monte Carlo in Practice",
  author    = "Gelfand, Alan E.",
  abstract  = "Responsible data analysis must address the issue of model
               determination, which we take as consisting of two components:
               model assessment or checking and model choice or selection.
               Since in practice, apart from rare situations, a model
               specification is never ‘correct’ we must ask (i) is a given
               model adequate? and (ii) within a collection of models under
               consideration, which is the best?",
  journal   = "Markov chain Monte Carlo in practice",
  publisher = "Springer US",
  pages     = "145--161",
  month     =  "1~" # jan,
  year      =  1996,
  isbn_alt  = "9781489944856"
}

@ARTICLE{Gelfand1992-ow,
  title     = "Model determination using predictive distributions with
               implementation via sampling-based methods",
  author    = "Gelfand, AE and Dey, DK and Chang, H",
  abstract  = "Abstract: Model determination is divided into the issues of
               model adequacy and model selection. Predictive distributions are
               used to address both issues. This seems natural since,
               typically, prediction is a primary purpose for the chosen model
               . A cross-validation ...",
  publisher = "DTIC Document",
  year      =  1992
}

@ARTICLE{Gelman1996-ez,
  title    = "Posterior predictive assessment of model fitness via realized
              discrepancies",
  author   = "Gelman, Andrew and Meng, Xiao-Li and Stern, Hal",
  journal  = "Stat. Sin.",
  volume   =  6,
  pages    = "733--807",
  year     =  1996,
  keywords = "and phrases; bayesian p -value; discrepancy; graphical assess-;
              ment; mixture model; model criticism; p -value; posterior
              predictive p -value; prior predictive; realized discrepancy; χ 2
              test"
}

@ARTICLE{Vehtari2012-oh,
  title    = "A survey of Bayesian predictive methods for model assessment,
              selection and comparison",
  author   = "Vehtari, Aki and Ojanen, Janne",
  journal  = "Stat. Surv.",
  volume   =  6,
  pages    = "142--228",
  year     =  2012,
  keywords = "62-02, 62C10Bayesian, predictive, model assessment; and phrases;
              bayesian; cross-validation; decision theory; expected utility;
              information cri-; model; model assessment; predictive; selection"
}

@ARTICLE{Geweke2004-yx,
  title   = "Getting It Right",
  author  = "Geweke, John",
  journal = "J. Am. Stat. Assoc.",
  volume  =  99,
  number  =  467,
  pages   = "799--804",
  month   =  sep,
  year    =  2004
}
@ARTICLE{noauthor_undated-xk,
  title = "A Bayesian formulation of exploratory data analysis"
}
@ARTICLE{Bayarri2007-cp,
  title     = "Bayesian Checking of the Second Levels of Hierarchical Models",
  author    = "Bayarri, M J and Castellanos, M E",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Stat. Sci.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  22,
  number    =  3,
  pages     = "322--343",
  month     =  aug,
  year      =  2007,
  keywords  = "Model checking; model criticism; objective Bayesian methods;
               p-values; conflict; empirical-Bayes; posterior predictive;
               partial posterior predictive",
  issn_alt  = "2168-8745"
}
@ARTICLE{Borgwardt2006-gy,
  title       = "Integrating structured biological data by Kernel Maximum Mean
                 Discrepancy",
  author      = "Borgwardt, Karsten M and Gretton, Arthur and Rasch, Malte J
                 and Kriegel, Hans-Peter and Sch{\"{o}}lkopf, Bernhard and
                 Smola, Alex J",
  affiliation = "Institute for Computer Science, Ludwig-Maximilians-University,
                 Munich, Germany. kb@dbs.ifi.lmu.de",
  abstract    = "MOTIVATION: Many problems in data integration in
                 bioinformatics can be posed as one common question: Are two
                 sets of observations generated by the same distribution? We
                 propose a kernel-based statistical test for this problem,
                 based on the fact that two distributions are different if and
                 only if there exists at least one function having different
                 expectation on the two distributions. Consequently we use the
                 maximum discrepancy between function means as the basis of a
                 test statistic. The Maximum Mean Discrepancy (MMD) can take
                 advantage of the kernel trick, which allows us to apply it not
                 only to vectors, but strings, sequences, graphs, and other
                 common structured data types arising in molecular biology.
                 RESULTS: We study the practical feasibility of an MMD-based
                 test on three central data integration tasks: Testing
                 cross-platform comparability of microarray data, cancer
                 diagnosis, and data-content based schema matching for two
                 different protein function classification schemas. In all of
                 these experiments, including high-dimensional ones, MMD is
                 very accurate in finding samples that were generated from the
                 same distribution, and outperforms its best competitors.
                 Conclusions: We have defined a novel statistical test of
                 whether two samples are from the same distribution, compatible
                 with both multivariate and structured data, that is fast, easy
                 to implement, and works well, as confirmed by our experiments.
                 AVAILABILITY: http://www.dbs.ifi.lmu.de/~borgward/MMD.",
  journal     = "Bioinformatics",
  volume      =  22,
  number      =  14,
  pages       = "e49--57",
  month       =  "15~" # jul,
  year        =  2006,
  issn_alt    = "1367-4803"
}
@ARTICLE{Gretton2007-ft,
  title     = "A kernel method for the two-sample-problem",
  author    = "Gretton, A and Borgwardt, K M and Rasch, M and {others}",
  abstract  = "Abstract We propose two statistical tests to determine if two
               samples are from different distributions. Our test statistic is
               in both cases the distance between the means of the two samples
               mapped into a reproducing kernel Hilbert space (RKHS). The first
               test is based ...",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "papers.nips.cc",
  year      =  2007
}
@ARTICLE{Guttman1967-my,
  title     = "The Use of the Concept of a Future Observation in
               {Goodness-of-Fit} Problems",
  author    = "Guttman, Irwin",
  abstract  = "An attack on the problem of goodness of fit is made by combining
               a Bayesian and sampling argument; the Bayesian part is effected
               by using the distribution of a future observation, while the
               sampling argument concerns itself with the distribution of a
               {"}chi-squared like{"} statistic, which measures discrepancies
               of observed frequencies from those predicted by the distribution
               of the future observation. Examples are given for the case of
               sampling from the binomial, Poisson and normal distributions. An
               interesting application arising from the above approach is a
               procedure for estimating the degree of a polynomial response
               function.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Wiley for the Royal Statistical Society",
  volume    =  29,
  number    =  1,
  pages     = "83--100",
  month     =  "1~" # jan,
  year      =  1967
}
@ARTICLE{Rosenblatt1956-hx,
  title     = "Remarks on Some Nonparametric Estimates of a Density Function",
  author    = "Rosenblatt, Murray",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Math. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  27,
  number    =  3,
  pages     = "832--837",
  month     =  sep,
  year      =  1956,
  issn_alt  = "2168-8990"
}
@ARTICLE{Parzen1962-hk,
  title     = "On estimation of a probability density function and mode",
  author    = "Parzen, E",
  abstract  = "The problem of estimation of a probability density function f
               (x) is interesting, for many reasons. As one possible
               application, we mention the problem of estimating the hazard, or
               conditional rate of failure, function /(ж)/1-F (x), where F (x)
               is the distribution function ...",
  journal   = "Ann. Math. Stat.",
  publisher = "ssg.mit.edu",
  year      =  1962
}
@BOOK{McLachlan2004-qz,
  title     = "Finite Mixture Models",
  author    = "McLachlan, G and Peel, D",
  abstract  = "An up-to-date, comprehensive account of major issues in finite
               mixture modeling This volume provides an up-to-date account of
               the theory and applications of modeling via finite mixture
               distributions. With an emphasis on the applications of mixture
               models in both ...",
  publisher = "Wiley",
  series    = "Wiley series in probability and statistics: Applied probability
               and statistics",
  year      =  2004
}
@ONLINE{deep-learning-tutorial,
title = "Deep learning tutorial",
year= "Accessed summer 2014",
url= "http://www.deeplearning.net/tutorial/"
}
@NOOK{lind2006basic,
  title={Basic statistics for business and economics},
  author={Lind, Douglas A and Marchal, William G and Wathen, Samuel Adam and Magazine, Business Week},
  year={2006},
  publisher={McGraw-Hill/Irwin Boston}
}
@inproceedings{WilAda13,
  title={Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation},
  author={Wilson, Andrew Gordon and Adams, Ryan Prescott},
  year={2013},
  booktitle={Proc. of 30th Int. Conf. on Machine Learning}
}
@ARTICLE{Benjamini_undated-mh,
  title  = "Controlling the False Discovery Rate: A Practical and Powerful
            Approach to Multiple Testing",
  author = "Benjamini, Yoav and Hochberg, Yosef"
}
@BOOK{Popper2005-ba,
  title     = "The logic of scientific discovery",
  author    = "Popper, K",
  abstract  = "Described by the philosopher AJ Ayer as a work of'great
               originality and power', this book revolutionized contemporary
               thinking on science and knowledge. Ideas such as the now
               legendary doctrine of'falsificationism'electrified the
               scientific community, influencing even ...",
  publisher = "books.google.com",
  year      =  2005
}
@ARTICLE{Cowles1996-qy,
  title     = "Markov Chain Monte Carlo Convergence Diagnostics: A Comparative
               Review",
  author    = "Cowles, Mary Kathryn and Carlin, Bradley P",
  abstract  = "A critical issue for users of Markov chain Monte Carlo (MCMC)
               methods in applications is how to determine when it is safe to
               stop sampling and use the samples to estimate characteristics of
               the distribution of interest. Research into methods of computing
               theoretical convergence bounds holds promise for the future but
               to date has yielded relatively little of practical use in
               applied work. Consequently, most MCMC users address the
               convergence problem by applying diagnostic tools to the output
               produced by running their samplers. After giving a brief
               overview of the area, we provide an expository review of 13
               convergence diagnostics, describing the theoretical basis and
               practical implementation of each. We then compare their
               performance in two simple models and conclude that all of the
               methods can fail to detect the sorts of convergence failure that
               they were designed to identify. We thus recommend a combination
               of strategies aimed at evaluating and accelerating MCMC sampler
               convergence, including applying diagnostic procedures to a small
               number of parallel chains, monitoring autocorrelations and
               cross-correlations, and modifying parametrizations or sampling
               algorithms appropriately. We emphasize, however, that it is not
               possible to say with certainty that a finite sample from an MCMC
               algorithm is representative of an underlying stationary
               distribution.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "American Statistical Association",
  volume    =  91,
  number    =  434,
  pages     = "883--904",
  month     =  "1~" # jun,
  year      =  1996
}
@ARTICLE{Gelman2003-xx,
  title     = "A Bayesian Formulation of Exploratory Data Analysis and
               Goodness‐of‐fit Testing*",
  author    = "Gelman, A",
  abstract  = "R\'{e}sum\'{e} Analyse de donn\'{e}es exploratrices et
               inf\'{e}rence (EDA) Bay\'{e}sienne (ou, en large,
               mod\'{e}lisation de statistiques complexes)-qui sont
               g\'{e}n\'{e}ralement consid\'{e}r\'{e}es comme \'{e}tant des
               paradigmes statistiques non relies. Dans cet article, nous
               pr\'{e}sentons un cadre pour l' ...",
  journal   = "Int. Stat. Rev.",
  publisher = "Wiley Online Library",
  year      =  2003
}
@ARTICLE{Milch2007-dz,
  title     = "1 {BLOG}: Probabilistic Models with Unknown Objects",
  author    = "Milch, B and Marthi, B and Russell, S and Sontag, D and {others}",
  abstract  = "Many AI problems, ranging from sensor data association to
               linguistic coreference resolution, involve making inferences
               about real-world objects that underlie some data. In many cases,
               we do not know the number of underlying objects or the mapping
               between observations ...",
  journal   = "Statistical relational",
  publisher = "books.google.com",
  year      =  2007
}
@ARTICLE{Koller1997-am,
  title     = "Effective Bayesian inference for stochastic programs",
  author    = "Koller, D and McAllester, D and Pfeffer, A",
  abstract  = "Abstract In this paper, we propose a stochastic version of a
               general purpose functional programming language as a method of
               modeling stochastic processes. The language contains random
               choices, conditional statements, structured values, defined
               functions, ...",
  journal   = "AAAI/IAAI",
  publisher = "aaai.org",
  year      =  1997
}