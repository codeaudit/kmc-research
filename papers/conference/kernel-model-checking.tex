\documentclass{article} % For LaTeX2e
\usepackage{format/nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{color}
\usepackage{preamble}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}
    
    
\usepackage{amsmath, amsfonts, bm, lipsum, capt-of}
\usepackage{natbib, xcolor, wrapfig, booktabs, multirow, caption}
\DeclareCaptionType{copyrightbox}
\usepackage{float}

%\renewcommand{\baselinestretch}{0.99}

\def\ie{i.e.\ }
\def\eg{e.g.\ }
\let\oldemptyset\emptyset
\let\emptyset\varnothing

\title{Using kernel two sample tests for\\statistical-model checking}

\author{
David S.~Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{1in}
\input{include/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  % Note, NA's pass through!

\begin{document} 

\maketitle

\begin{abstract} 
We investigate the utility of the maximum mean discrepancy (MMD) two sample test as a means of statistical-model checking.
MMD two sample tests not only provide a means for hypothesis testing, but also show where any discrepancies are most extreme via their witness function.
We demonstrate on synthetic data that the witness function can be used to identify where a statistical model most misrepresents the data it was trained on.
We then demonstrate the procedure on real data using restricted Boltzmann machines, deep belief networks and [something else] and demonstrate the ways in which these models are failing to capture the properties of the data they are trained on.
\end{abstract} 

\allowdisplaybreaks

\section{Introduction}

Model checking, assessment or criticism is an important part of a complete statistical analysis.
For example, when one fits a linear model to a data set via least squares a complete analysis includes computing Cook's distances (\NA{cite}) to identify influential points and plotting residuals against fitted values to identify non-linearity or heteroscedasticity.
Similarly, modern approaches to Bayesian modelling (\NA{cite BDA}) view model checking as in important components of an iterative cycle of model construction, inference and checking.

As statistical models become more complex and diverse in response to the challenges of modern data sets there will be an increasing need for a greater range of model checking procedures that are either automatic or generally applicable.
This will be especially true as automatic modelling methods (\NA{cite Roger, us and some automatic data mining if I can find it}) and probabilistic programming (\NA{cite}) mature.

Model checking typically proceeds by choosing a statistic of interest, computing it on data and comparing this to a suitable null distribution or posterior predictive distribution.
Ideally these statistics are chosen to assess the utility of the statistical model under consideration but this can require considerable expertise on the part of the modeller.

We propose an alternative to this approach by using a statistic defined as a supremum over a broad class of measures of discrepancy between two distributions, the maximum mean discrepancy (MMD)  (\NA{cite}).
The advantage of this approach is that the discrepancy measure attaining the supremum automatically identifies regions of the data which are least well represented by the statistical model fit to the data.

We demonstrate this approach to model checking on synthetic data sets and two real world examples.
\NA{Some sort of call to arms about a lack of model checking in the machine learning literature}.

\section{Background: Model checking and maximum mean discrepancy}

In this section we relate poster predictive $p$-values with maximum mean discrepancy (MMD)  statistics.

\subsection{Posterior predictive $p$-values}

Suppose we observe data $Y = (y_i)_{i=1\ldots n}$ and we attempt to fit a model $M$ with parameters $\theta$.
After performing a statistical analysis we will have either an estimate, $\hat\theta$, or an (approximate) posterior, $p(\theta|M,Y)$, for the parameters.

The posterior predictive distribution over replicate data $Y^\textrm{rep}$ is given by
\begin{equation}
p(Y^\textrm{rep}|M,Y) = \int p(Y^\textrm{rep}|M,\theta)p(\theta|M,Y)\mathrm{d}\theta
\end{equation}
where the posterior distribution for $\theta$ may be replaced by a point mass in a frequentist analysis.
In a Bayesian context this represents the belief over potential future data sets generated by the same mechanism as the first data set after having observed the first data set.

Posterior predictive $p$-values (cite Rubin 1984, Meng 1994, Gelman 1996) are defined by\fTBD{check notation is up to date}
\begin{equation}
p_b(Y) = \mathbb{P}(T(Y^\textrm{rep})\geq T(Y)|M,Y)
\end{equation}
where $T$ is a statistic \ie a function of the data.
This is the posterior probability that a potential future data set will be more extreme than the observed data as measured by the statistic $T$.
Small $p$-values indicate that the observed data is extreme compared to the posterior predictive distribution, indicating a lack of fit between the posterior distribution and the data.

\subsection{Posterior predictive $p$-values for exchangeable data and their connection with mean discrepancy}

We now consider the situation where we can assume that our data $Y$ are $\iid$ samples from some distribution.
This is appropriate whenever we are invariant to the order of our data \ie when the data is exchangeable (\NA{cite deFinetti, Savage possibly and Dan + Peter}).

To demonstrate the connection between posterior predictive $p$-values and maximum mean discrepancy measures we rewrite the $p$-value as an expectation
\begin{equation}
\mathbb{E}(\mathbb{I}[T(Y^\textrm{rep}) - T(Y) > 0]).
\end{equation}
We now remove the indicator function from this expression
\begin{equation}
  \mathbb{E}(T(Y^\textrm{rep})) - T(Y)
\end{equation}
yielding a measure of how the posterior mean of the statistic compares to the statistic evaluated on the data.
If we assume that our statistic $T$ is of the form $T(Y) = \sum t(y_i)$ (this includes \eg all empirical moments) then
\begin{equation}
\mathbb{E}(T(Y^\textrm{rep})) - T(Y) = \sum \mathbb{E}(t(y_i^\textrm{rep})) - \sum t(y_i) = \mathbb{E}(t(y^\textrm{rep})) - \mathbb{E}(t(y))
\end{equation}
where $y$ and $y^\textrm{rep}$ are distributed according to the empirical distribution of the data and the posterior-predictive distribution over single data points respectively.
This quantity is called a mean discrepancy between the distribution of $y$ and $y^\textrm{rep}$.

In the practical application of posterior-predictive $p$-values, statistics are chosen to measure aspects of the data that are of relevance to the analysis being performed.
Whilst there are numerous examples (\NA{cite some applied papers}) of sensible choices for standard models, there is little work on exploratory model checking.
The benefit of considering mean discrepancies is that we can analytically maximise over a large space of different statistics.

\TBD{
Can we relate any of this to utility theory.
Using scoring rules to justify the use of the posterior predictive.
}

\subsubsection{Alternative maths}

In this setting it is natural to replace the posterior predictive distribution over data sets with the posterior predictive distribution over data points
\begin{equation}
p(y^\textrm{rep}|M,Y) = \int p(y^\textrm{rep}|M,\theta)p(\theta|M,Y)\mathrm{d}\theta.
\end{equation}
We then define posterior predictive $p$-values as
\begin{equation}
  p_\textrm{x}(Y) = \mathbb{P}_{y\sim \hat{Y}}(T(y^\textrm{rep})\geq T(y)|M,Y) = \mathbb{E}_{y\sim \hat{Y}}(\mathbb{I}[T(y^\textrm{rep}) - T(y) > 0])
\end{equation}
where $\hat{Y}$ if the empirical distribution of the data.
This is a measure of how extreme the data is compared to the posterior predictive distribution over data points.
If we remove the indicator function in the above equation we get
\begin{equation}
  \mathbb{E}(T(y^\textrm{rep})) - \mathbb{E}(T(y))
\end{equation}
which measures the mean discrepancy between the posterior predictive distribution and the data as measured by the statistic $T$.

\subsection{Maximum mean discrepancy}

Consider the two sample problem \ie given samples $X = (x_i)_{i=1\ldots m}$ and $Y = (y_i)_{i=1\ldots n}$ drawn \iid from distributions $p$ and $q$ respectively, can we determine if $p \neq q$?

An answer to this problem is to consider maximum mean discrepancy (MMD) (cite Gretton) statistics (also called integral probability metrics by Mueller 1997)
\begin{equation}
\textrm{MMD}(\mathcal{F},p,q) = \sup_{f \in \mathcal{F}}(\mathbb{E}_{x\sim p}[f(x)] - \mathbb{E}_{y\sim q}[f(y)])
\end{equation}
where $\mathcal{F}$ is a set of functions.
These types of statistics are of interest since they are computed at the function that extremises the discrepancy between two distributions, subject to the constraints of the function space $\mathcal{F}$.

When $\mathcal{F}$ is a reproducing kernel Hilbert space (RKHS) the function attaining the supremum can be derived analytically and is called the witness function
\begin{equation}
f(x) = \mathbb{E}_{x'\sim p}[k(x,x')] - \mathbb{E}_{x'\sim q}[k(x,x')]
\end{equation}
where $k$ is the kernel of the RKHS.
Consequently the MMD can be expressed without the supremum as
%\begin{eqnarray}
%\textrm{MMD}^2(\mathcal{F},p,q) & = & \phantom{-2}\mathbb{E}_{x,x'\sim p}[k(x,x')] \nonumber\\
%&& - 2\mathbb{E}_{x\sim p,y\sim p}[k(x,y)] \nonumber\\
%&& + \phantom{2}\mathbb{E}_{y,y'\sim q}[k(y,y')]
%\end{eqnarray}
\begin{equation}
  \textrm{MMD}^2(\mathcal{F},p,q) = \mathbb{E}_{x,x'\sim p}[k(x,x')] + 2\mathbb{E}_{x\sim p,y\sim p}[k(x,y)] + \mathbb{E}_{y,y'\sim q}[k(y,y')]
\end{equation}
for which one can form a simple biased estimate
%\begin{eqnarray}
%\textrm{MMD}_b^2(\mathcal{F},X,Y) & = & \phantom{+}\frac{1}{m^2}\sum_{i,j=1}^{m}k(x_i,x_j) \nonumber\\
%&& - \frac{2}{mn}\sum_{i,j=1}^{m,n}k(x_i,y_j) \nonumber\\
%&& + \frac{1}{n^2}\sum_{i,j=1}^{n}k(y_i,y_j)
%\end{eqnarray}
\begin{equation}
  \textrm{MMD}_b^2(\mathcal{F},X,Y) = \frac{1}{m^2}\sum_{i,j=1}^{m}k(x_i,x_j) - \frac{2}{mn}\sum_{i,j=1}^{m,n}k(x_i,y_j) + \frac{1}{n^2}\sum_{i,j=1}^{n}k(y_i,y_j)
\end{equation}
with which one can construct statistical tests of whether or not $p=q$.

The witness function can be estimated from finite samples similarly
\begin{equation}
\hat{f}(x) = \frac{1}{m}\sum_{i=1}^{m}k(x,x_i) - \frac{1}{n}\sum_{i=1}^{n}k(x,y_i).
\end{equation}

From this final equation we can see that the empirical witness function is the difference of two kernel density estimates (cite Nadaraya and Watson probably).
\TBD{Say more about this for interpretability.}

\section{MMD for posterior predictive checking}

In the previous section we demonstrated that posterior-predictive $p$-values are related to the mean discrepancy between the posterior-predictive distribution and empirical distribution of the data.
We therefore propose using kernel MMD two-sample tests as a means of statistical-model checking.

The appeal of using kernel two sample tests is that we get a $p$-value determining statistical significance of observed discrepancies, but we also get an estimate of the witness function, which shows us where the two distributions differ most, allowing a more exploratory form of model checking.

To summarise the difference of approaches, the posterior predictive $p$-values ask the question `Is the data extreme compared to my posterior as measured by a particular statistic?'.
Instead we are asking the question `Is it plausible that the data could have been generated by my posterior?'.
\NA{We discuss the philosophy behind asking these questions in a later section.}

\subsection{Kernel choice}

The nature of the two sample test defined by the kernel MMD depends on the choice of the kernel.
In this paper we use the radial basis function kernel, also known as the squared exponential or exponentiated quadratic (\NA{cite standard things}).
\NA{Say what this kernel encodes for and assumes and mention that alternatives exist}.

\NA{
Lengthscale also important, discuss the median distance heuristic and other possible methods (and relation of MMD to average discrepancy under GP prior).
Also discuss the idea of using the lengthscale that results in the best kernel density estimates.
}

\subsection{Estimation of null distribution}

\NA{
We use the bootstrap.
But fancier methods exist when one has lots of data (cite the latest machinery).
}

\paragraph{Noto bene}

\TBD{
Model checking should not replace model comparison and averaging and posterior predictive $p$-values determine \emph{statistical} significance, not \emph{practical} significance.
Maybe this can become a larger philosophy section.
}

\section{Examples on synthetic data}

\subsection{Newcomb's speed of light data}

\TBD{nanseconds $\to$ nanoseconds}

A histogram of Simon Newcomb's (cite - Stigler 1977 possibly) 66 measurements used to determine the speed of light is shown on the left of figure~\ref{fig:newcomb}.
We consider fitting a normal distribution to this data by maximum likelihood\footnotemark.
\footnotetext{66 data points in one dimension is strongly informative so maximum likelihood will give very similar results to a fully Bayesian treatment with weakly informative priors}

\begin{figure}[ht]
\centering
\includegraphics[width=0.32\columnwidth]{figures/newcomb_hist}
\includegraphics[width=0.32\columnwidth]{figures/newcomb_witness_1}
\includegraphics[width=0.32\columnwidth]{figures/newcomb_witness_2}
\caption{
Left: Histogram of Simon Newcomb's speed of light measurements.
Middle: Histogram together with density estimate (red solid line) and MMD witness function (green dashed line).
Right: Histogram together with improved density estimate (red solid line) and MMD witness function (green dashed line).
}
\label{fig:newcomb}
\end{figure}

To perform a MMD two sample test we sampled 1000 points from the fitted distribution, used the median heuristic to select a lengthscale and estimated the null distribution using 1000 bootstrap replications.
The estimated $p$-value of the test was less than 0.001 \ie a clear disparity between the model and data.
The data, fitted density estimate (the normal distribution) and witness function are shown in the middle of figure~\ref{fig:newcomb}.
The witness function has a trough at the centre of the data and peaks either side.
This indicates that the fitted model has placed too little mass in its centre and too much mass outside its centre.
We can therefore speculate that the data is more leptokurtic or has smaller variance than the fitted model.

This suggests that we should modify our model by either allowing for heavy tails or explicitly modelling the possibility of outliers which could have resulted in the variance being over-estimated.
However, to demonstrate some of the properties of the MMD statistic we make an unusual choice of fitting a Gaussian by maximum likelihood, but ignoring the two outliers in the data.
The new fitted density estimate (the normal distribution) and witness function of an MMD test are shown in the middle of figure~\ref{fig:newcomb}.
The estimated $p$-value associated with the MMD two sample test is close to 0.5, despite the fitted model being a very poor explanation of the outliers.
This demonstrates that the MMD test using a radial basis function kernel can identify dense discrepancies, rather than outliers.
However, methods that are not robust to outliers will likely show dense discrepancies that will be identified by the test.

%\subsection{Spotting an unusual cluster}
%
%Figure~\ref{fig:blob_blob_ring} shows synthetic 2 dimensional data generated from two Gaussians and a uniform ring density (green circles).
%
%\begin{figure}[ht]
%\centering
%\includegraphics[width=0.98\columnwidth]{figures/blob_blob_ring}
%\caption{
%An experiment on synthetic data revealing a statistically significant maximum mean discrepancy.
%Green $\circ$ are data, red $\times$ are locations over-represented by the model, blue $+$ are locations under-represented by the model.
%}
%\label{fig:blob_blob_ring}
%\end{figure}
%
%We fit a mixture of Gaussians (cite) to this data using 3 centres.
%The kernel two sample test using the median lengthscale heuristic returns a $p$-value of (insert number here) indicating that no discrepancies can be identifed at this lengthscale.
%However, plotting a histogram of all pairwise distances over the data reveals two predominant scales in the data; the median is closest to the larger value.
%
%Performing a kernel two sample test with the smaller lengthscale results in a $p$-value of (insert number) indicating a discrepancy between the model and the data.
%Similar to before we can identify this discrepancy via the witness function.
%\TBD{
%Something about extreme points or basins of attraction or a plot of the witness function.
%}
%The witness function has clearly identified the uniform ring of data points as being inconsistent with the mixture of Gaussians model.
%
%\TBD{Follow up could be trying to fit this with more Gaussians - how many do we need to patch it up - will there always be discrepancies or will they become too small to notice?}

\subsection{High dimensional data}

The interpretability of the witness functions comes from being equivalent to the difference of two kernel density estimates.
In high dimensional spaces, kernel density estimation is a very high variance procedure that can result in poor density estimates (\NA{is there a classic citation}) which will destroy the interpretability of the method.
Note however that the statistical test derived from the MMD still has high power in high dimensions (\NA{cite Gretton}).

To test how the MMD statistic can be used for high dimensional data we generated synthetic data using the following recipe.
5 points in a 10 dimensional space were drawn at random from a random 4 dimensional subspace\footnotemark.
\footnotetext{The details are not especially important; code for replication will be available upon publication}
Data was generated as isotropic Gaussian distributions centred on 4 of the 5 points.
Finally, data was centred on the fifth point drawn from an isotropic $t$-distribution with 2 degrees of freedom.
In sum, the data is a mixture of Gaussians and a $t$-distribution.

We then fit a mixture of Gaussians with 5 centres to the data and then generated samples from the fitted distribution in order to perform an MMD two sample test.
To ensure that the witness function can be safely interpreted as the difference of two density estimates we reduced the dimensionality of the data using principal component analysis (PCA), selecting the first two principal components.
The data and posterior predictive samples are plotted on the left of figure~\ref{fig:high_mog}.
While we can see that one cluster is different from the rest, it is difficult to assess by eye if these distributions are different due in part to the difficulty of plotting two sets of samples on top of each other.

\begin{figure}[ht]
\centering
\includegraphics[width=0.4\columnwidth]{figures/high_mog_fa}
\hspace{0.1\columnwidth}
\includegraphics[width=0.4\columnwidth]{figures/high_mog_witness}
\caption{
Left: PCA projection of synthetic high dimensional cluster data (green circles) and projection of samples from fitted model (red circles).
Right: Witness function of MMD two sample test. The erroneously fit cluster is clearly identified.
Note: The scales are not the same on both plots, this will be remedied.
}
\label{fig:high_mog}
\end{figure}

Using the median heuristic to select a lengthscale results in a test that returns a $p$-value of [something], indicating that the test has not identified any discrepancies.
Indeed, the lengthscale chosen by this heuristic is [something] which is of the order of the distances between clusters.
The test therefore is blind to discrepancies smaller than distances between clusters, and since the mixture of Gaussian has correctly identified the 5 centres and sizes of the mixture distribution, the test does not find any discrepancies.

However, taking the density estimate interpretation of the witness function more seriously suggests choosing lengthscales that result in the best density estimates.
We therefore selected a lengthscale by 5 fold cross validation using predictive likelihood of the kernel density estimate as the selection criterion.
With this lengthscale the MMD test returns a $p$-value of [something] and the witness function (right of figure~\ref{fig:high_mog}) clearly identifies the cluster that has been incorrectly modelled.

Presented with this discrepancy identification a statistical modeller might try a more flexible cluster model (a mixture of $t$-distributions would work) (\NA{cite my favourite clustering methods \eg spectral, Dave}).
However, the $p$-value of the MMD statistic can also be made non-significant by fitting a mixture of 10 Gaussians.
We mention this as a reminder that model checking cannot assess the `truth' of a model, only whether the data could plausibly have been generated by the model (or if the data is not extreme under the model is using posterior predictive $p$-values).

\section{Applications to real data and complex statistical models}

\subsection{What exactly do neural networks dream about?}

``To recognize shapes, first learn to generate images'' \cite{Hinton2007}.
Restricted Boltzmann Machine (RBM) pretraining of neural networks was shown by \cite{Hinton2006} to learn a deep belief network (DBN) (cite) for the training data \ie a generative model.
Subsequently, as well as computing estimates of marginal likelihood and testing errors, it became standard to demonstrate the effectiveness of a neural network by generating samples from the distribution it had learned\fTBD{cite things}.

When trained on the MNIST handwritten digit data, the samples certainly look like digits, but it is hard to detect any systematic anomalies purely by visual inspection.
We now use the kernel two-sample test to investigate how faithfully RBMs and DBNs can capture the probability distribution over handwritten digits.

\subsubsection{RBMs prefer rounded numbers}

We trained an RBM with architecture $(784)\leftrightarrow(500)\leftrightarrow(10)$ using 15 epochs of PCD-15, a batch size of (something) and a learning rate of 0.1 (\ie we used the same settings as the code available at (cite deep learning tutorial)).
We generated 3000 independent samples from the learned generative model by initialising the network with a random training image and performing 1000 clamped gibbs updates (without clamping the label pixels, the generative distribution is biased towards certain digits) to generate each image (this is standard \eg \cite{Hinton2007}).

Figure~\ref{fig:rbm_samples} shows the first twenty random samples from this model.
They look like digits, but has the true distribution over digits been faithfully captured?
A priori the answer to this question is almost certainly no, but it is not immediately obvious how the learned distribution will deviate from the true distribution.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/rbm_samples}
\caption{
Samples from an RBM (actually the mean activations).
}
\label{fig:rbm_samples}
\end{figure}

PCA (factor analysis too slow) shows clear discrepancy between fitted and true distribution.
Fancy stats not really required.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/rbm_pca}
\caption{
A caption.
}
\label{fig:rbm_pca}
\end{figure}

But we can summarise the difference using the peaks of the witness function, ordering them by their contribution to the MMD (left to right).
On the top we have 29\%, 23\%, 11\%, 6\%, 3\%\ldots and on the bottom we have 5\%, 2\%, 2\%, 2\%, 2\%.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/rbm_witness_peaks}
\caption{
A caption.
}
\label{fig:rbm_witness_peaks}
\end{figure}

These percentages indicate that the MMD is mostly coming from dense regions of the fantasy digits whearas the true distribution is more spread out.

Now we look at samples from many RBMs.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/many_rbm_pca}
\caption{
A caption.
}
\label{fig:many_rbm_pca}
\end{figure}

This is better but by eye some discrepancies are still visible and the MMD is still significant.
Witness function shows the density discrepancies more clearly.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/many_rbm_witness}
\caption{
A caption.
}
\label{fig:many_rbm_witness}
\end{figure}

Now the summary of peaks and troughs.
20\%, 14\%, 6\%, 4\%, 3\% and 16\%, 8\%, 7\%, 5\%, 3\%.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/many_rbm_witness_peaks}
\caption{
A caption.
}
\label{fig:many_rbm_witness_peaks}
\end{figure}

It is a bit more balanced.
Hard to understand exactly what is happening / what these digits mean.

Actually it prefers zero for some reason, looking at peaks and troughs of conditional distributions we get.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/many_rbm_cond}
\caption{
A caption.
}
\label{fig:many_rbm_cond}
\end{figure}

That is, it really likes zero!

DBN PCA.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/dbn_pca}
\caption{
A caption.
}
\label{fig:dbn_pca}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/dbn_witness_peaks}
\caption{
A caption.
}
\label{fig:dbn_witness_peaks}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/dbn_cond}
\caption{
A caption.
}
\label{fig:dbn_cond}
\end{figure}

DBN with fine tuning.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/dbn_ft_pca}
\caption{
A caption.
}
\label{fig:dbn_ft_pca}
\end{figure}

Clearly it is getting closer to being a good model.
But MMD still detects a significant difference.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/dbn_ft_witness_peaks}
\caption{
A caption.
}
\label{fig:dbn_ft_witness_peaks}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/dbn_ft_cond}
\caption{
A caption.
}
\label{fig:dbn_ft_cond}
\end{figure}

Looks like it is suffering from ghosting.

But if we go into 3 dimensions our conclusions might change.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/dbn_ft_3_witness_peaks}
\caption{
A caption.
}
\label{fig:dbn_ft_witness_peaks}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/dbn_ft_3_cond}
\caption{
A caption.
}
\label{fig:dbn_ft_cond}
\end{figure}

In 5 dimensions we get way too many peaks and troughs to be useful.

But a consistent thing is that it likes the `1' that it sometimes thinks is a `4'.
This looks like the zero thing but less extreme.

\TBD{OLD MATERIAL BELOW}

Performing a kernel two sample test using the median length scale heuristic resulted in an estimated $p$-value less than 0.001 \ie a clear discrepancy between the model and the data.
We can investigate this discrepancy by examining the images at which the witness function is extremised.
The most over/under represented images are displayed in figure~\ref{fig:rbm_over_under}.
The RBM appears to over-represent rounded 2/3/9 whilst it under-represents slanted 3, 5 and 8.

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\columnwidth]{figures/rbm_over_under}
\caption{
Over (top row) and under (bottom row) represented images by the RBM.
}
\label{fig:rbm_over_under}
\end{figure}

To test that this was not just a poorly trained single RBM, we trained 1500 RBMs and generated one sample from each and performed the same tests.
The estimated $p$-value was 0.005.
We show the over/under represented images in figure~\ref{fig:many_rbm_over_under}.

\TBD{
The over-represented images are somewhat 'ghosted' - \ie it looks like multiple images on top of each other.
In contrast therefore the under-represented images are fat and bold.
This is prabably due to 'mistakes' in the sampling process in higher levels resulting in incorrect activations of several neurons in lower levels.
Not entirely sure how to best probe this - to be discussed.
}

\TBD{
I am currently trying to generatively fine tune the dbn using a CD algorithm proposed by Geoff Hinton - need to have a very small learning rate - work in progress.
}

\subsection{Pitman-Yor processes say the darndest things}

Look http://www.sequencememoizer.com/ and http://www.gatsby.ucl.ac.uk/~ucabjga/code.html

\subsection{Related work and discussion of model checking}

We may base our work on model checks but they are not accepted by all.

Philosophy of model checking.
If one has a strongly informative prior and weakly informative data then we do not expect our posterior to always look like the data - this criticism applies equally to posterior predictive checks.

\section{Conclusions and future work}

Do you know what your model is up to right now?
Maybe you should check it is ok.

Can also apply to Geweke tests.

Future - graphs, functions\ldots

\bibliography{gpss, library}
\bibliographystyle{format/icml2014}

\end{document} 
